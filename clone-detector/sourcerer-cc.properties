#Sourcerer-CC-D specific
NODE_PREFIX=NODE
QUERY_DIR_PATH=input/dataset
OUTPUT_DIR=${NODE_PREFIX}/output
CANDIDATES_DIR=test/tensorFlow/experiments/results/scalability/candidates
DATASET_DIR_PATH=input/dataset
MAPPING_FILE=input/mapping/IjaMapping_new_uniquetokens.txt
TOKENS_MAPPING_FILE=input/mapping/Method_Token_Map.txt
CLONE_PAIRS_FILE=input/mapping/clonepairs.txt
IS_GEN_CANDIDATE_STATISTICS=false
IS_STATUS_REPORTER_ON=true
#for recovery
LOG_PROCESSED_LINENUMBER_AFTER_X_LINES=50

#for sockets
START_PORT=9900
END_PORT=9903
ADDRESS=localhost
#350mb
SOCKET_BUFFER=350000000
#1MB
DEFAULT_BUFFERED_WRITER_SIZE=1000000
IS_TRAIN_MODE=false
# Ignore all files outside these bounds
#num_tokens,num_unique_tokens,num_statements,num_expressions,num_assignments
METRICS_ORDER_IN_SHARDS=num_tokens
IS_SHARDING=true

# Sharding speeds up search for very large datasets (>200K files).
# For small-ish datasets, it doesn't matter so much

SEARCH_SHARDS=ALL

# Tier 1 shards based on num tokens
LEVEL_1_MIN_TOKENS=15
LEVEL_1_MAX_TOKENS=500000
LEVEL_1_SHARD_MAX_NUM_TOKENS=30,60,120,300,600

#Tier 2 shards based on num unique tokens
#LEVEL_2_MIN_TOKENS=1
#LEVEL_2_MAX_TOKENS=5000
#LEVEL_2_SHARD_MAX_NUM_TOKENS=5,47,65,91,141

#Tier 3 shards based on num of statements
#LEVEL_3_MIN_TOKENS=0
#LEVEL_3_MAX_TOKENS=10000
#LEVEL_3_SHARD_MAX_NUM_TOKENS=22,57

#Tier 4 shards based on num of expresions
#LEVEL_4_MIN_TOKENS=0
#LEVEL_4_MAX_TOKENS=10000
#LEVEL_4_SHARD_MAX_NUM_TOKENS=70

#Tier 5 shards based on num of assignments
#LEVEL_5_MIN_TOKENS=0
#LEVEL_5_MAX_TOKENS=10000
#LEVEL_5_SHARD_MAX_NUM_TOKENS=10


# The next few variables serve for tuning performance.
# Their values depend, in part, on how many cores are available.
# The default values work well for 1 single SourcererCC process 
# on an 8-core machine.
# in gigabytes
MAX_INDEX_SIZE=6
# INDEXING
BTSQ_THREADS=1
BTIIQ_THREADS=1
BTFIQ_THREADS=1

# SEARCH
QLQ_THREADS=1
QBQ_THREADS=4
QCQ_THREADS=10
VCQ_THREADS=1
RCQ_THREADS=1
